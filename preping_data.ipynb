{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import geonamescache\n",
    "from collections import Counter\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"ml_insurance_challenge.csv\",encoding=\"ISO-8859-1\")\n",
    "#print(data.head())\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "#data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9494, 5)"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "string.punctuation\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    punctuationfree = \"\".join([i for i in text if i not in string.punctuation])\n",
    "    return punctuationfree\n",
    "\n",
    "clean=data.copy()\n",
    "clean['description'] = data['description'].apply(lambda x: remove_punctuation(str(x)))\n",
    "clean['business_tags'] = data['business_tags'].apply(lambda x: remove_punctuation(str(x)))\n",
    "\n",
    "#clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anama_1lv\\AppData\\Local\\Temp\\ipykernel_7708\\2595729430.py:1: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  lower = clean.applymap(lambda x: x.lower() if isinstance(x, str) else x)\n"
     ]
    }
   ],
   "source": [
    "lower = clean.applymap(lambda x: x.lower() if isinstance(x, str) else x)\n",
    "\n",
    "#lower.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "lower['merged'] = lower[['description', 'business_tags', 'sector', 'category', 'niche']].apply(lambda x: ' '.join(x.dropna()), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anama_1lv\\AppData\\Local\\Temp\\ipykernel_7708\\4222538184.py:7: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  tokens = lower.applymap(lambda x: tokenization(x))\n"
     ]
    }
   ],
   "source": [
    "# def tokenization(text):\n",
    "#     if not isinstance(text, str):\n",
    "#         text = str(text)\n",
    "#     tokens = re.findall(r'\\w+', text)\n",
    "#     return tokens\n",
    "\n",
    "# tokens = lower.applymap(lambda x: tokenization(x))\n",
    "# #tokens.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anama_1lv\\AppData\\Local\\Temp\\ipykernel_7708\\3256859301.py:6: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  tokens_no_stopwords = tokens.applymap(lambda x: remove_stopwords(x))\n"
     ]
    }
   ],
   "source": [
    "# stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# def remove_stopwords(tokens):\n",
    "#     return [word for word in tokens if word.lower() not in stop_words]\n",
    "\n",
    "# tokens_no_stopwords = tokens.applymap(lambda x: remove_stopwords(x))\n",
    "\n",
    "# # tokens_no_stopwords.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def remove_first_n_words(text, n):\n",
    "#     if isinstance(text, list):\n",
    "#         return text[n:]\n",
    "#     return text\n",
    "\n",
    "# tokens_no_stopwords['description'] = tokens_no_stopwords['description'].apply(lambda x: remove_first_n_words(x, 4))\n",
    "\n",
    "# tokens_no_stopwords.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# countries = [country['name'].lower() for country in geonamescache.GeonamesCache().get_countries().values()]\n",
    "# countries.append('uk') \n",
    "# cities = [city['name'].lower() for city in geonamescache.GeonamesCache().get_cities().values()]\n",
    "\n",
    "# def remove_cities(text):\n",
    "#     if isinstance(text, list):\n",
    "#         return [word for word in text if word not in cities]\n",
    "#     return text\n",
    "\n",
    "# def remove_countries(text):\n",
    "#     if isinstance(text, list):\n",
    "#         return [word for word in text if word not in countries]\n",
    "#     return text\n",
    "\n",
    "# tokens_no_stopwords['merged'] = tokens_no_stopwords['merged'].apply(remove_cities).apply(remove_countries)\n",
    "\n",
    "# # tokens_no_stopwords.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def is_english_word(word):\n",
    "#     return wordnet.synsets(word)\n",
    "\n",
    "# def remove_non_english_words(tokens):\n",
    "#     return [word for word in tokens if is_english_word(word)]\n",
    "\n",
    "# tokens_english = tokens_no_stopwords.copy()\n",
    "# tokens_english['merged'] = tokens_no_stopwords['merged'].apply(remove_non_english_words)\n",
    "\n",
    "# # tokens_english.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_tokens = [word for sublist in tokens_no_stopwords['merged'] for word in sublist]\n",
    "# word_counts = Counter(all_tokens)\n",
    "\n",
    "# def remove_most_common_words(tokens, n):\n",
    "#     if isinstance(tokens, list):\n",
    "#         sorted_words = sorted(tokens, key=lambda word: -word_counts[word])\n",
    "#         removed_words = sorted_words[:n]\n",
    "#         # print(f\"Removed words: {removed_words}\")\n",
    "#         return sorted_words[n:]\n",
    "#     return tokens\n",
    "\n",
    "# # def remove_least_common_words(tokens, n):\n",
    "# #     if isinstance(tokens, list):\n",
    "# #         sorted_words = sorted(tokens, key=lambda word: word_counts[word])\n",
    "# #         removed_words = sorted_words[:n]\n",
    "# #         print(f\"Removed words: {removed_words}\")\n",
    "# #         return sorted_words[n:]\n",
    "# #     return tokens\n",
    "\n",
    "# def calculate_n_to_remove(tokens, n):\n",
    "#     length = len(tokens)\n",
    "#     return length // n\n",
    "\n",
    "# tokens_filtered = tokens_english.copy()\n",
    "# tokens_filtered['merged'] = tokens_english['merged'].apply(lambda x: remove_most_common_words(x, calculate_n_to_remove(x, 5)))\n",
    "\n",
    "# # tokens_filtered['description'] = tokens_filtered['description'].apply(lambda x: remove_least_common_words(x, calculate_n_to_remove(x, 15)))\n",
    "\n",
    "# # tokens_filtered.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokens_filtered.to_csv(\"preped_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def remove_duplicates(tokens):\n",
    "#     if isinstance(tokens, list):\n",
    "#         return list(dict.fromkeys(tokens))\n",
    "#     return tokens\n",
    "\n",
    "# tokens_no_duplicates = tokens_filtered.applymap(lambda x: remove_duplicates(x))\n",
    "\n",
    "# # tokens_no_duplicates.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokens_filtered=tokens_filtered.drop_duplicates(subset=['merged'], keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_filtered.to_csv(\"preped_data.csv\", index=False, mode='w')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
